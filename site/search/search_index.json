{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VisorCV","text":""},{"location":"#the-premier-computer-vision-application-framework","title":"The premier Computer Vision Application Framework","text":""},{"location":"#why-visorcv","title":"Why VisorCV?","text":"<p>Building real-time computer vision applications presents a common set of challenges for developers. This framework is designed to handle the common tasks and let you focus on building your product.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Just like a web application framework abstracts away the common HTTP implementation, VisorCV provides the core set of APIs required to build your computer vision app. All with best practices and high performance in mind so you don't have to worry about dropping frames or lagging behind. Just focus on your computer vision algorithms and busines logic and let VisorCV handle the rest.</p> <p> </p>"},{"location":"#what-is-it","title":"What is it?","text":"<p>VisorCV provides the best developer experience to develop any computer vision application. Building on top of the leading computer vision (OpenCV) and machine learning (PyTorch) libraries, supporting a number of runtimes (ONNXRuntime, Hugging Face), and pretrained/custom models (YOLOv8, Detectron2) all with scalable deployment in mind (Docker). VisorCV will get your product ready for production in no time.</p>"},{"location":"#install","title":"Install","text":""},{"location":"#pip","title":"Pip","text":"<pre><code>pip install visorcv\n</code></pre>"},{"location":"#docker","title":"Docker","text":"<pre><code>docker pull visorml/visorcv:latest\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#create-a-computer-vision-app","title":"Create a computer vision app","text":"<p>Create a new computer vision application named <code>cv-project</code>. <pre><code>visorcv new cv-project\n</code></pre></p>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>project-name/\n\u251c\u2500 app/\n\u2502  \u251c\u2500 consumers/\n\u2502  \u2502   \u2514\u2500 camera_stream.py\n\u2502  \u251c\u2500 models/\n\u2502  \u2502  \u2514\u2500 yolov8n.onnx\n\u2502  \u251c\u2500 predictors/\n\u2502  \u2502  \u2514\u2500 yolo_detector.py\n\u2502  \u2514\u2500 views/\n\u2502     \u2514\u2500 blurred_faces.py\n\u251c\u2500 config/\n\u2502  \u251c\u2500 cameras.yml\n\u2502  \u2514\u2500 mkdocs.yml\n\u251c\u2500 tests/\n\u2502  \u251c\u2500 consumers\n\u2502  \u2514\u2500 mkdocs.yml\n\u2514\u2500 requirements.txt\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Readers <ul> <li>VideoReader</li> </ul> </li> <li>Writers (Producer since it produces files/RTSP streams? But it consumes from frame streams...)<ul> <li>FileWriter()</li> <li>StreamWriter(Broadcaster)</li> </ul> </li> <li>Predictors<ul> <li>Detector</li> <li>Segmentor</li> <li>Classifier</li> <li>Tracker</li> </ul> </li> <li>Results<ul> <li>Detection</li> <li>Segementation</li> <li>Classification</li> <li>Tracklet</li> </ul> </li> <li>Annotators(Views?)<ul> <li>Box</li> <li>Mask</li> <li>Label</li> <li>Blur</li> </ul> </li> <li>Model</li> <li>Transformer</li> </ul>"},{"location":"#integrations","title":"Integrations","text":""},{"location":"#pytorch","title":"PyTorch","text":""},{"location":"#_1","title":"VisorCV","text":"<pre><code> model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol>"},{"location":"#hugging-face","title":"Hugging Face","text":"<pre><code>from huggingface_hub import hf_hub_download\n</code></pre>"},{"location":"#ray","title":"Ray","text":""},{"location":"#predictors","title":"Predictors","text":"<p>Predictors are inference classes that take <code>image_data</code> and output disignated <code>results</code> that can be annotated based on the <code>result</code> - YOLOv8 - Custom ONNX or PyTorch Models</p>"},{"location":"#video-consumer","title":"Video Consumer","text":"<pre><code>sequenceDiagram\n    autonumber\n    Video Stream --&gt;&gt; Video Consumer: Read Frames\n    Video Consumer --&gt;&gt; Ray Frame Buffer: Write Frames to Buffer\n    Video Consumer --&gt;&gt; Faktory: Equeue Predictor job by frame id</code></pre>"},{"location":"#predictor","title":"Predictor","text":"<pre><code>sequenceDiagram\n    autonumber\n    Faktory --&gt;&gt; Predictor: Dequeue Predictor jobs\n    Ray Frame Buffer --&gt;&gt; Predictor: Read Frames from Buffer\n    Predictor --&gt;&gt; Redis: Write Results to Redis\n    Predictor --&gt;&gt; Faktory: Enqueue render job to faktory</code></pre>"},{"location":"#resultrenderer-renders-results","title":"ResultRenderer: Renders results","text":"<pre><code>sequenceDiagram\n    autonumber\n    Faktory --&gt;&gt; Result Renderer: Dequeue render job\n    Redis --&gt;&gt; Result Renderer: Read results from redis\n    Ray Frame Buffer --&gt;&gt; Result Renderer: Read frame from buffer\n    Note over Result Renderer: render results to frame</code></pre>"},{"location":"#resultrenderer-enqueues-video-production","title":"ResultRenderer: Enqueues video production","text":"<pre><code>sequenceDiagram\n    autonumber\n    Ray Frame Buffer --&gt;&gt; Result Renderer: Write rendered frame to Buffer\n    Result Renderer --&gt;&gt; Redis: Write Results to Redis\n    Result Renderer --&gt;&gt; Faktory: Enqueue video production job to faktory</code></pre>"},{"location":"#videoproducer-produces-video-files-and-streams","title":"VideoProducer: Produces video files and streams","text":"<pre><code>sequenceDiagram\n    autonumber\n    Faktory --&gt;&gt; Video Producer: Dequeue video production job\n    Redis --&gt;&gt; Video Producer: Read results from redis\n    Video Producer --&gt;&gt; Video: write frame to File/RTSP</code></pre> <pre><code>sequenceDiagram\n    autonumber\n    vp --&gt;&gt; API: report results to API\n    HTTP --&gt;&gt; User: User requests live feed\n    RTSP --&gt;&gt; User: Watches live feed</code></pre> <pre><code>sequenceDiagram\n    participant vs as Video File/Stream\n    participant vc as Video Consumer\n    destroy vs\n    vs -&gt;&gt;+ vc: Read Frames\n    participant rfb as Ray Frame Buffer\n    vc -&gt;&gt;+ rfb: Write Frames to Buffer\n    create participant f as Fraktory\n    vc -)- f: Equeue Predictor job by frame id\n    create participant ap as Async Predictor\n    f -&gt;&gt;+ ap: Dequeue Predictor jobs\n    rfb -&gt;&gt;+ ap: Read Frames from Buffer\n    Note over ap: Run inference\n    create participant r as Redis\n    ap -&gt;&gt;- r: Write Results to Redis\n    destroy ap\n    ap -) f: Enqueue render job to faktory\n    create participant vp as Video Producer\n    f -) vp: Dequeue render job\n    r -&gt;&gt; vp: Read results from redis\n    rfb -&gt;&gt; vp: Read frame from buffer\n    create participant RTSP\n    vp -&gt;&gt; RTSP: write frame to RTSP\n    create participant File\n    vp -&gt;&gt; File: write frame to file\n    create participant API\n    vp -&gt;&gt; API: report results to API\n    create actor User\n    HTTP -&gt;&gt; User: User requests live feed\n    RTSP -&gt;&gt; User: Watches live feed</code></pre>"},{"location":"#generate-inference-classes","title":"Generate inference classes","text":"<p>These generators will create the appropriate python code and download the supporting models.</p> <p>Generate a Detector from an off the shelf model <pre><code>visorcv generate detector --model=yolov8\n</code></pre> Generate a detector for a customer model <pre><code>visorcv generate detector --model=path/to/your/model.onnx\n</code></pre> Generate a Segmentor from an off the shelf model <pre><code>visorcv generate segmentor --model=yolov8\n</code></pre> Generate a Classifier from an off the shelf model <pre><code>visorcv generate classifier --model=yolov8\n</code></pre></p> <p>Supported Models</p> <ul> <li>PyTorch</li> <li>ONNXRuntime<ul> <li>CoreML models</li> <li>TensorRT models</li> </ul> </li> </ul> <p>Generate a Tracker from an off the shelf model <pre><code>visorcv generate tracker --model=bytetracker --config=botsort.yaml\n</code></pre></p> <p>Supported Trackers   - BoT-SORT - Use botsort.yaml to enable this tracker.   - ByteTrack - Use bytetrack.yaml to enable this tracker.</p>"},{"location":"#updating-models","title":"Updating models","text":""},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#optimizations","title":"Optimizations","text":"<ul> <li> BetterTransformer for multiGPU inference?</li> </ul>"},{"location":"#offlinecloud-platforms","title":"Offline/Cloud Platforms","text":"<ul> <li> Metaflow?</li> </ul>"},{"location":"#platforms","title":"Platforms","text":"<ul> <li> MacOS Apple Silicon Support with CoreMLTools</li> </ul>"},{"location":"#docker-images","title":"Docker Images","text":"<ul> <li> NVIDIA GPU support with CUDA, CuDNN, PyTorch, and ONNXRuntime</li> </ul>"},{"location":"#distributed-platforms","title":"Distributed Platforms","text":"<ul> <li> Ray Object Store</li> <li> Docker Swarm</li> </ul>"},{"location":"#embedded-platforms","title":"Embedded Platforms","text":"<ul> <li> Raspberry Pi support <ul> <li> Disk image</li> <li> Docker image</li> <li> Reading from Pi Cam</li> <li> Streaming via RTSP</li> </ul> </li> <li> Jetson support<ul> <li> Disk image</li> <li> Docker image</li> </ul> </li> </ul>"},{"location":"diagrams/workflow/","title":"Workflow","text":"<pre><code>graph TD\n    F(Faktory)\n    R(Redis)\n    RFB(Ray Frame Store)\n    RTSP(RTSP Server)\n\n    CV[Video Consumer] \n    CV--&gt; |writes frame| RFB\n    CV --&gt; |enqueue predictor job| F\n\n    AP[AsyncPredictors]\n    RFB --&gt; |read frames| AP\n    AP --&gt; |Run Inference| AP\n    AP --&gt; |Return Results to Redis| R\n    AP --&gt; |enqueue render result job| F\n\n    RR[Render Results]\n    F --&gt; |dequeue render result job| RR\n    R --&gt; |read results| RR\n    RFB --&gt; |read frames| RR\n    RR --&gt; DBM[Draw Boxes/Masks]\n    DBM --&gt; RL[Render Labels]\n    RL --&gt; AB[Apply Blur]\n    RR--&gt; |writes rendered frame| RFB\n    VP --&gt; |enqueue video production job| F\n\n    VP[Video Producer]\n    F --&gt; |dequeue video production job| VP\n    RFB --&gt; |read rendered frames| AP\n    VP --&gt; |write rendered frame| RTSP</code></pre>"}]}